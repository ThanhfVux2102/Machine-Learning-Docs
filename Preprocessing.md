## ğŸ“˜ Preprocessing Cheatsheet by Data Type

| ğŸ“‚ Data Type | âš™ï¸ Main Preprocessing Steps | ğŸ› ï¸ Common Methods / Functions | ğŸ” Recognition Trick |
|-------------|-----------------------------|-------------------------------|-----------------------|
| ğŸ–¼ï¸ Image | 1. Resize<br>2. Normalize pixel values<br>3. Data Augmentation<br>4. Encode labels | `tf.image.resize()`<br>`tf.cast(img, tf.float32)/255`<br>`ImageDataGenerator`,<br>`tf.image.random_flip_left_right`<br>`LabelEncoder`, `to_categorical()` | `.jpg`, `.png`, shape `(H, W, C)` |
| ğŸ“„ Text | 1. Lowercase<br>2. Remove punctuation / stopwords<br>3. Tokenize<br>4. Padding<br>5. Vectorization (TF-IDF / BERT) | `.lower()`, `re.sub()`<br>`nltk.corpus.stopwords`, `re`<br>`Tokenizer().fit_on_texts()`<br>`pad_sequences()`<br>`TfidfVectorizer`, `BERTTokenizer` | Files like `.txt`, `.csv` containing sentences or paragraphs |
| ğŸ“Š Tabular | 1. Handle missing values<br>2. Encode categorical variables<br>3. Scale numerical features<br>4. Feature selection | `SimpleImputer`, `dropna()`<br>`OneHotEncoder`, `LabelEncoder`, `get_dummies()`<br>`StandardScaler`, `MinMaxScaler`<br>`SelectKBest` | CSV/XLS with multiple columns and data types |
| ğŸ“ˆ Time-Series | 1. Parse timestamp<br>2. Resampling<br>3. Normalize<br>4. Sliding Window | `pd.to_datetime()`<br>`df.resample('D').mean()`<br>`StandardScaler`, `MinMaxScaler`<br>`sliding_window_view` | Has `"timestamp"` column, time-sequenced data |
| ğŸ”Š Audio | 1. Feature extraction (MFCC, Spectrogram)<br>2. Normalize<br>3. Noise reduction<br>4. Sampling | `librosa.feature.mfcc()`<br>`librosa.amplitude_to_db()`<br>`scipy.signal`,<br>`noisereduce.reduce_noise()`<br>`librosa.resample()` | `.wav`, `.mp3` files or waveform arrays |
| ğŸï¸ Video | 1. Extract frames<br>2. Resize frames<br>3. Normalize<br>4. Sampling | `cv2.VideoCapture().read()`<br>`cv2.resize()`<br>`frame/255.0`<br>Sampling by FPS or time | `.mp4`, `.avi` files or image sequences |



## Goal for pipeline for every Data Type :

| âœ… **Step**                | âœ… **Purpose / Description**                                                                 |
|---------------------------|----------------------------------------------------------------------------------------------|
| **1. Load the dataset**   | Read and import the raw data into the pipeline (from disk, database, web, etc.)              |
| **2. Normalize the data** | Standardize or scale the data (e.g. pixel values to [0, 1], text tokenization, feature scaling) |
| **3. Shuffle the data**   | Randomize the order of data to prevent model overfitting or learning from sequence bias      |
| **4. Batch the data**     | Divide the data into fixed-size groups (batches) to enable efficient training/inference      |
| **5. Prefetch the data**  | Load batches ahead of time during training to improve pipeline throughput and efficiency     |


# ğŸ¯ TensorFlow Dataset Pipeline â€“ Tham sá»‘ & CÃ¡ch chá»n (Cheatsheet)

## 1. `.map(function, num_parallel_calls=...)`

| Tham sá»‘              | Khi nÃ o cáº§n chá»‰nh                              | CÃ¡ch chá»n                                                                 |
|----------------------|------------------------------------------------|---------------------------------------------------------------------------|
| `function`           | **Báº¯t buá»™c**                                   | HÃ m xá»­ lÃ½ tá»«ng pháº§n tá»­ (normalize, augment, tokenize, scale...)          |
| `num_parallel_calls` | Khi dataset lá»›n hoáº·c xá»­ lÃ½ náº·ng trong `map()`  | DÃ¹ng `tf.data.AUTOTUNE` Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t                              |

> ğŸ’¡ Tip: LuÃ´n dÃ¹ng `num_parallel_calls=tf.data.AUTOTUNE` khi xá»­ lÃ½ áº£nh, text hoáº·c augment nhiá»u.

---

## 2. `.shuffle(buffer_size, seed=None, reshuffle_each_iteration=True)`

| Tham sá»‘                    | Khi nÃ o cáº§n chá»‰nh                                           | CÃ¡ch chá»n                                                                 |
|----------------------------|-------------------------------------------------------------|---------------------------------------------------------------------------|
| `buffer_size`              | Ráº¥t quan trá»ng Ä‘á»ƒ trÃ¡nh overfitting                        | - Báº±ng sá»‘ máº«u (VD: `ds_info.splits['train'].num_examples`)                |
| `seed`                     | Muá»‘n káº¿t quáº£ reproducible                                  | Chá»n sá»‘ cá»¥ thá»ƒ nhÆ° `seed=42`                                              |
| `reshuffle_each_iteration`| Náº¿u muá»‘n dá»¯ liá»‡u shuffle láº¡i má»—i epoch                     | Máº·c Ä‘á»‹nh lÃ  `True`, chá»‰ táº¯t khi debug                                     |

> âš ï¸ Shuffle quÃ¡ yáº¿u â†’ model há»c theo thá»© tá»± vÃ  overfit.

---

## 3. `.batch(batch_size, drop_remainder=False)`

| Tham sá»‘          | Khi nÃ o cáº§n chá»‰nh                                     | CÃ¡ch chá»n                                               |
|------------------|-------------------------------------------------------|---------------------------------------------------------|
| `batch_size`     | LuÃ´n cáº§n chá»‰ Ä‘á»‹nh                                     | - GPU yáº¿u: 16â€“32 <br> - GPU máº¡nh: 64â€“128+               |
| `drop_remainder` | Muá»‘n cÃ¡c batch cÃ³ shape Ä‘á»“ng nháº¥t                     | `True` náº¿u dÃ¹ng RNN/LSTM hoáº·c TPU yÃªu cáº§u fixed shape   |

> âœ… Náº¿u dÃ¹ng mÃ´ hÃ¬nh yÃªu cáº§u input cá»‘ Ä‘á»‹nh â†’ nÃªn dÃ¹ng `drop_remainder=True`.

---

## 4. `.prefetch(buffer_size)`

| Tham sá»‘          | Khi nÃ o cáº§n chá»‰nh                   | CÃ¡ch chá»n                          |
|------------------|-------------------------------------|------------------------------------|
| `buffer_size`    | Muá»‘n tÄƒng tá»‘c Ä‘á»™ training           | DÃ¹ng `tf.data.AUTOTUNE`            |

> ğŸ’¡ NÃªn luÃ´n cÃ³ `.prefetch(tf.data.AUTOTUNE)` á»Ÿ cuá»‘i pipeline.

---

## 5. `.repeat(count=None)`

| Tham sá»‘     | Khi nÃ o cáº§n chá»‰nh                         | CÃ¡ch chá»n                           |
|-------------|-------------------------------------------|-------------------------------------|
| `count`     | Khi báº¡n muá»‘n dataset tá»± láº·p láº¡i nhiá»u láº§n | Náº¿u dÃ¹ng trong `.fit(epochs=...)`, Ä‘á»ƒ máº·c Ä‘á»‹nh lÃ  vÃ´ háº¡n (`None`) |

---

## ğŸ§  NguyÃªn táº¯c tÆ° duy chá»n tham sá»‘:

| CÃ¢u há»i                        | Gá»£i Ã½ hÃ nh Ä‘á»™ng                                       |
|-------------------------------|--------------------------------------------------------|
| Dataset lá»›n hay nhá»?          | Dataset lá»›n â†’ dÃ¹ng `shuffle(buffer_size lá»›n)`         |
| GPU cÃ³ cháº­m khÃ´ng?            | DÃ¹ng `prefetch(tf.data.AUTOTUNE)`                     |
| CÃ³ cáº§n xá»­ lÃ½ song song?       | DÃ¹ng `map(..., num_parallel_calls=tf.data.AUTOTUNE)`  |
| MÃ´ hÃ¬nh cáº§n shape cá»‘ Ä‘á»‹nh?    | DÃ¹ng `drop_remainder=True` trong `.batch()`           |

---

## ğŸ” Cáº¥u trÃºc pipeline tiÃªu chuáº©n:

```python
ds_train = ds_train.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.shuffle(buffer_size=10000, seed=42)
ds_train = ds_train.batch(32, drop_remainder=True)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

