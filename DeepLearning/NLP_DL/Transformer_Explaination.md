# üìò Chi Ti·∫øt T·ª´ng Step + C√°ch Ch·ªçn Tham S·ªë + Ch·ªçn Model Cho NLP Transformer Level 3

T√†i li·ªáu n√†y gi·∫£i th√≠ch r√µ t·ª´ng b∆∞·ªõc trong FLOW NLP Transformer Level 3, ƒë·ªìng th·ªùi h∆∞·ªõng d·∫´n c√°ch ch·ªçn m√¥ h√¨nh, ch·ªçn tham s·ªë, v√† g·ª£i √Ω setup cho t·ª´ng d·∫°ng b√†i.

---

# üü¶ STEP 1 ‚Äî Chu·∫©n B·ªã D·ªØ Li·ªáu (Dataset Preparation)

### ‚úî M·ª•c ƒë√≠ch  
T·∫°o ngu·ªìn d·ªØ li·ªáu s·∫°ch v√† ƒë√∫ng format, nh∆∞ng **kh√¥ng x·ª≠ l√Ω qu√° nhi·ªÅu**, v√¨ Transformer t·ª± hi·ªÉu ng·ªØ c·∫£nh.

### ‚úî Ch·ªâ c·∫ßn l√†m:
- x√≥a k√Ω t·ª± l·ªói  
- normalize spacing  
- b·ªè tag HTML  
- lo·∫°i emoji n·∫øu b√†i to√°n kh√¥ng c·∫ßn  

### ‚úî Kh√¥ng c·∫ßn l√†m:
- b·ªè stopwords  
- stemming  
- lemmatization  
- lower-case b·∫Øt bu·ªôc (m·ªôt s·ªë model ph√¢n bi·ªát hoa‚Äìth∆∞·ªùng)  

### üî• Tips:  
N·∫øu d·ªØ li·ªáu l√† social media ‚Üí c·∫ßn l·ªçc b·ªõt k√Ω t·ª± spam nh∆∞: @#$%^&*, URL.

---

# üü© STEP 2 ‚Äî Train / Validation / Test Split

### ‚úî T·ªâ l·ªá chu·∫©n
- **70 / 15 / 15** (d·ªØ li·ªáu l·ªõn)  
- **80 / 10 / 10** (d·ªØ li·ªáu nh·ªè)  

### ‚úî Quan tr·ªçng: Stratify
N·∫øu l√† classification ‚Üí lu√¥n stratify theo label.

---

# üü¶ STEP 3 ‚Äî Tokenization (HuggingFace)

Transformer d√πng **subword tokenization**, v√≠ d·ª• ‚Äúplaying‚Äù ‚Üí ‚Äúplay‚Äù + ‚Äú##ing‚Äù.

### ‚úî C√°ch ch·ªçn max_length:
- 64 ‚Üí comment, tweet  
- 128 ‚Üí review, m√¥ t·∫£ ng·∫Øn  
- 256 ‚Üí b√°o, m√¥ t·∫£ d√†i  
- 512 ‚Üí t√†i li·ªáu, report  

### üî• L∆∞u √Ω:  
max_length c√†ng l·ªõn ‚Üí RAM c√†ng t·ªën ‚Üí training ch·∫≠m h∆°n.

---

# üü® STEP 4 ‚Äî T·∫°o Dataset PyTorch

Dataset ph·∫£i tr·∫£ v·ªÅ:
- input_ids  
- attention_mask  
- labels  

V·ªõi NER ‚Üí tr·∫£ v·ªÅ labels theo t·ª´ng token (sequence labeling).

---

# üüß STEP 5 ‚Äî Load Model Transformer

### ‚úî Khi n√†o ch·ªçn BERT-base?
- b√†i to√°n ti·∫øng Anh  
- d·ªØ li·ªáu trung b√¨nh  
- accuracy ·ªïn ƒë·ªãnh  
- m√¥ h√¨nh ph·ªï th√¥ng  

### ‚úî Khi n√†o ch·ªçn DistilBERT?
- m√°y y·∫øu  
- mu·ªën t·ªëc ƒë·ªô nhanh  
- x√¢y app mobile/web  
- inference realtime  

### ‚úî Khi n√†o ch·ªçn RoBERTa?
- c·∫ßn accuracy cao  
- d·ªØ li·ªáu ph·ª©c t·∫°p  
- GPU m·∫°nh h∆°n  

### ‚úî Khi n√†o ch·ªçn XLM-R?
- d·ªØ li·ªáu ti·∫øng Vi·ªát  
- multilingual  
- vƒÉn b·∫£n lai nhi·ªÅu ng√¥n ng·ªØ  

---

# üü¶ STEP 6 ‚Äî Fine-tuning Model

### ‚úî Ch·ªçn Learning Rate
- **2e-5** ‚Üí chu·∫©n nh·∫•t  
- **3e-5** ‚Üí nhanh h∆°n, d·ªÖ overfit  
- **5e-5** ‚Üí ch·ªâ d√πng cho model nh·ªè nh∆∞ DistilBERT  

### ‚úî Batch size
- 16 ‚Üí GPU 8GB  
- 32 ‚Üí GPU 12GB  
- 64 ‚Üí GPU 24GB+  

### ‚úî Epoch
- 2 ‚Üí khi dataset l·ªõn  
- 3 ‚Üí chu·∫©n  
- 4 ‚Üí khi dataset nh·ªè  

### ‚úî Warmup ratio
- 10% s·ªë step ‚Üí gi√∫p training m∆∞·ª£t h∆°n

### ‚úî Weight Decay
- 0.01 ‚Üí chu·∫©n cho Transformer  

### üî• M·∫πo quan tr·ªçng:
RoBERTa th∆∞·ªùng c·∫ßn nhi·ªÅu epoch h∆°n BERT.

---

# üü¶ STEP 7 ‚Äî Model Evaluation

### Classification
- Accuracy  
- Precision / Recall  
- Macro-F1 **(quan tr·ªçng nh·∫•t n·∫øu m·∫•t c√¢n b·∫±ng)**  
- Confusion Matrix  

### NER
- F1 theo entity-level  

### Sentence Similarity
- Cosine similarity  
- Spearman correlation  

---

# üü¶ STEP 8 ‚Äî Inference Pipeline

### ‚úî M·ª•c ti√™u  
T·∫°o h√†m d·ª± ƒëo√°n ƒë∆°n gi·∫£n, nh·∫≠n input ‚Üí tr·∫£ label.

### ‚úî L∆∞u √Ω:
- Lu√¥n tokenize v·ªõi padding/truncation  
- Transformer tr·∫£ logits ‚Üí softmax ‚Üí label  

---

# üü¶ STEP 9 ‚Äî Save Model

Bao g·ªìm:
```
config.json  
pytorch_model.bin  
tokenizer.json  
special_tokens_map.json  
```

### ‚úî Khi c·∫ßn ONNX?
- ch·∫°y CPU  
- ch·∫°y real-time  
- deploy mobile  

---

# üü¶ STEP 10 ‚Äî Deploy API

### Framework:
- **FastAPI** ‚Üí production  
- **Streamlit/Gradio** ‚Üí demo nhanh  

### ‚úî C√≥ c·∫ßn GPU server kh√¥ng?
Ch·ªâ khi:
- model > 200M params  
- t·ªëc ƒë·ªô d·ª± ƒëo√°n > 50 req/s  

---

# üü¶ STEP 11 ‚Äî README / Report

N·ªôi dung chu·∫©n:
- m√¥ t·∫£ task  
- m√¥ t·∫£ dataset  
- EDA  
- l√Ω do ch·ªçn m√¥ h√¨nh  
- hyperparameters  
- training logs  
- evaluation metrics  
- confusion matrix  
- error analysis  
- future work  

---

# üìå C√ÅCH CH·ªåN MODEL THEO B√ÄI TO√ÅN

### ‚úî Sentiment Analysis
- DistilBERT ‚Üí n·∫øu mu·ªën nhanh  
- BERT-base ‚Üí stable  
- RoBERTa ‚Üí accuracy cao nh·∫•t  

### ‚úî Hate Speech / Toxic Comments
- RoBERTa  
- XLM-R (n·∫øu c√≥ ti·∫øng Vi·ªát)  

### ‚úî Text Classification (news/product review)
- BERT-base  
- XLM-R (multilingual)  

### ‚úî NER
- BERT-base  
- XLM-R ‚Üí t·ªët cho ti·∫øng Vi·ªát  

### ‚úî Semantic Similarity
- SBERT  
- Siamese-BERT  

---

# üìå C√ÅCH CH·ªåN THAM S·ªê NHANH (CHEAT-SHEET)

| Th√†nh ph·∫ßn | Gi√° tr·ªã chu·∫©n |
|-----------|---------------|
| Learning rate | **2e-5** |
| Batch size | **16** |
| Epoch | **3** |
| Max length | **128** |
| Warmup | **10% steps** |
| Weight decay | **0.01** |

---


