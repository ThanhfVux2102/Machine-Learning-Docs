# üìù Tutorial: Machine Learning Pipeline with Scikit-learn

## 1. Data Splitting

### `train_test_split` (from `sklearn.model_selection`)
- Chia d·ªØ li·ªáu th√†nh t·∫≠p **train** v√† **test**.

```python
train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
```

- **`test_size`**: t·ª∑ l·ªá d·ªØ li·ªáu test (v√≠ d·ª•: 0.2 = 20%).
- **`random_state`**: seed c·ªë ƒë·ªãnh, ƒë·∫£m b·∫£o t√°i l·∫≠p k·∫øt qu·∫£.
- **`stratify`**: n·∫øu = `y`, gi·ªØ ƒë√∫ng t·ª∑ l·ªá nh√£n khi chia train/test.

---

## 2. Data Normalization / Scaling

Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë gi√∫p m√¥ h√¨nh h·ªôi t·ª• nhanh h∆°n, tr√°nh dominance c·ªßa feature l·ªõn.

### Ph∆∞∆°ng ph√°p ph·ªï bi·∫øn:
- **`StandardScaler`**: chu·∫©n h√≥a d·ªØ li·ªáu v·ªÅ mean=0, std=1.  
- **`MinMaxScaler`**: scale d·ªØ li·ªáu v·ªÅ [0,1].  
- **`RobustScaler`**: √≠t nh·∫°y v·ªõi outliers, d·ª±a tr√™n median v√† IQR.  
- **`Normalizer`**: chu·∫©n h√≥a theo vector norm (L1, L2).  

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer
```

---

## 3. Encoding (Categorical Features)

Bi·∫øn ƒë·ªïi d·ªØ li·ªáu d·∫°ng chu·ªói th√†nh s·ªë.

### C√°c ph∆∞∆°ng ph√°p:
- **`OneHotEncoder`**: t·∫°o dummy variables (0/1) cho t·ª´ng h·∫°ng m·ª•c.
- **`LabelEncoder`**: √°nh x·∫° m·ªói gi√° tr·ªã th√†nh 1 s·ªë nguy√™n (ch·ªâ d√πng cho target ho·∫∑c khi feature c√≥ th·ª© t·ª±).
- **`OrdinalEncoder`**: g√°n s·ªë theo th·ª© t·ª± cho feature d·∫°ng ordinal (v√≠ d·ª•: small < medium < large).
- **`Target Encoding`** (ngo√†i sklearn, d√πng th∆∞ vi·ªán `category_encoders`): thay h·∫°ng m·ª•c b·∫±ng mean c·ªßa target.

```python
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
```

---

## 4. ColumnTransformer

K·∫øt h·ª£p nhi·ªÅu b∆∞·ªõc x·ª≠ l√Ω cho t·ª´ng lo·∫°i c·ªôt.

```python
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ],
    remainder='drop'
)
```

- **`remainder='drop'`**: b·ªè c·ªôt kh√¥ng x·ª≠ l√Ω.
- **`remainder='passthrough'`**: gi·ªØ nguy√™n c·ªôt kh√¥ng x·ª≠ l√Ω.

---

## 5. Model Options

### a) Classification
- **`SGDClassifier(loss="log_loss")`**: Logistic Regression v·ªõi Gradient Descent.
- **`LogisticRegression`**: m√¥ h√¨nh logistic regression chu·∫©n.
- **`RandomForestClassifier`**: c√¢y quy·∫øt ƒë·ªãnh ensemble, m·∫°nh m·∫Ω, √≠t c·∫ßn scaling.
- **`SVC` (Support Vector Machine)**: t·ªët v·ªõi d·ªØ li·ªáu kh√¥ng tuy·∫øn t√≠nh.
- **`KNeighborsClassifier`**: d·ª±a tr√™n kho·∫£ng c√°ch, ƒë∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£.

### b) Regression
- **`SGDRegressor`**: Linear Regression b·∫±ng Gradient Descent.
- **`LinearRegression`**: m√¥ h√¨nh tuy·∫øn t√≠nh chu·∫©n.
- **`RandomForestRegressor`**: ensemble trees cho regression.
- **`SVR`**: Support Vector Regression.
- **`KNeighborsRegressor`**: t∆∞∆°ng t·ª± KNN nh∆∞ng cho regression.

---

## 6. Pipeline

D√πng `Pipeline` ƒë·ªÉ gom t·∫•t c·∫£ b∆∞·ªõc l·∫°i.

```python
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier

clf = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('classifier', SGDClassifier(loss="log_loss", max_iter=1000, tol=1e-3, random_state=42))
])
```

- **`steps`**: list `(t√™n b∆∞·ªõc, object)`.
- C√°c b∆∞·ªõc ƒë·∫ßu l√† preprocessing, b∆∞·ªõc cu·ªëi l√† model.

---

## 7. Evaluation Metrics

### Classification
```python
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
```

- **`accuracy_score`**: t·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng.
- **`confusion_matrix`**: ma tr·∫≠n d·ª± ƒëo√°n.
- **`classification_report`**: Precision, Recall, F1-score.

### Regression
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
```

- **`MSE`**: trung b√¨nh b√¨nh ph∆∞∆°ng sai s·ªë.
- **`MAE`**: trung b√¨nh sai s·ªë tuy·ªát ƒë·ªëi.
- **`R2`**: h·ªá s·ªë x√°c ƒë·ªãnh (1 l√† t·ªët nh·∫•t).

---

## 8. Visualization

```python
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
```

---

## ‚úÖ Summary

- **Splitting**: `train_test_split` ƒë·ªÉ chia d·ªØ li·ªáu.  
- **Scaling**: ch·ªçn gi·ªØa Standard, MinMax, Robust, Normalizer.  
- **Encoding**: ch·ªçn OneHot, Label, Ordinal, ho·∫∑c Target Encoding.  
- **Model**: ch·ªçn Classification (SGD, Logistic, RF, SVM, KNN) ho·∫∑c Regression (SGDRegressor, Linear, RF, SVR, KNN).  
- **Pipeline**: gom c√°c b∆∞·ªõc l·∫°i ƒë·ªÉ code g·ªçn v√† √≠t l·ªói.  
- **Evaluation**: metrics cho classification ho·∫∑c regression.  
- **Visualization**: confusion matrix, learning curve, feature importance.  
