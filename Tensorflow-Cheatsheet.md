# üéØ TensorFlow Dataset Pipeline ‚Äì Tham s·ªë & C√°ch ch·ªçn (Cheatsheet)

## 1. `.map(function, num_parallel_calls=...)`

| Tham s·ªë              | Khi n√†o c·∫ßn ch·ªânh                              | C√°ch ch·ªçn                                                                 |
|----------------------|------------------------------------------------|---------------------------------------------------------------------------|
| `function`           | **B·∫Øt bu·ªôc**                                   | H√†m x·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠ (normalize, augment, tokenize, scale...)          |
| `num_parallel_calls` | Khi dataset l·ªõn ho·∫∑c x·ª≠ l√Ω n·∫∑ng trong `map()`  | D√πng `tf.data.AUTOTUNE` ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t                              |

> üí° Tip: Lu√¥n d√πng `num_parallel_calls=tf.data.AUTOTUNE` khi x·ª≠ l√Ω ·∫£nh, text ho·∫∑c augment nhi·ªÅu.

---

## 2. `.shuffle(buffer_size, seed=None, reshuffle_each_iteration=True)`

| Tham s·ªë                    | Khi n√†o c·∫ßn ch·ªânh                                           | C√°ch ch·ªçn                                                                 |
|----------------------------|-------------------------------------------------------------|---------------------------------------------------------------------------|
| `buffer_size`              | R·∫•t quan tr·ªçng ƒë·ªÉ tr√°nh overfitting                        | - B·∫±ng s·ªë m·∫´u (VD: `ds_info.splits['train'].num_examples`)                |
| `seed`                     | Mu·ªën k·∫øt qu·∫£ reproducible                                  | Ch·ªçn s·ªë c·ª• th·ªÉ nh∆∞ `seed=42`                                              |
| `reshuffle_each_iteration` | N·∫øu mu·ªën d·ªØ li·ªáu shuffle l·∫°i m·ªói epoch                     | M·∫∑c ƒë·ªãnh l√† `True`, ch·ªâ t·∫Øt khi debug                                     |

> ‚ö†Ô∏è Shuffle qu√° y·∫øu ‚Üí model h·ªçc theo th·ª© t·ª± v√† overfit.

---

## 3. `.batch(batch_size, drop_remainder=False)`

| Tham s·ªë          | Khi n√†o c·∫ßn ch·ªânh                                     | C√°ch ch·ªçn                                               |
|------------------|-------------------------------------------------------|---------------------------------------------------------|
| `batch_size`     | Lu√¥n c·∫ßn ch·ªâ ƒë·ªãnh                                     | - GPU y·∫øu: 16‚Äì32 <br> - GPU m·∫°nh: 64‚Äì128+               |
| `drop_remainder` | Mu·ªën c√°c batch c√≥ shape ƒë·ªìng nh·∫•t                     | `True` n·∫øu d√πng RNN/LSTM ho·∫∑c TPU y√™u c·∫ßu fixed shape   |

> ‚úÖ N·∫øu d√πng m√¥ h√¨nh y√™u c·∫ßu input c·ªë ƒë·ªãnh ‚Üí n√™n d√πng `drop_remainder=True`.

---

## 4. `.prefetch(buffer_size)`

| Tham s·ªë          | Khi n√†o c·∫ßn ch·ªânh                   | C√°ch ch·ªçn                          |
|------------------|-------------------------------------|------------------------------------|
| `buffer_size`    | Mu·ªën tƒÉng t·ªëc ƒë·ªô training           | D√πng `tf.data.AUTOTUNE`            |

> üí° N√™n lu√¥n c√≥ `.prefetch(tf.data.AUTOTUNE)` ·ªü cu·ªëi pipeline.

---

## 5. `.repeat(count=None)`

| Tham s·ªë     | Khi n√†o c·∫ßn ch·ªânh                         | C√°ch ch·ªçn                           |
|-------------|-------------------------------------------|-------------------------------------|
| `count`     | Khi b·∫°n mu·ªën dataset t·ª± l·∫∑p l·∫°i nhi·ªÅu l·∫ßn | N·∫øu d√πng trong `.fit(epochs=...)`, ƒë·ªÉ m·∫∑c ƒë·ªãnh l√† v√¥ h·∫°n (`None`) |

---

## üß† Nguy√™n t·∫Øc t∆∞ duy ch·ªçn tham s·ªë:

| C√¢u h·ªèi                        | G·ª£i √Ω h√†nh ƒë·ªông                                       |
|-------------------------------|--------------------------------------------------------|
| Dataset l·ªõn hay nh·ªè?          | Dataset l·ªõn ‚Üí d√πng `shuffle(buffer_size l·ªõn)`         |
| GPU c√≥ ch·∫≠m kh√¥ng?            | D√πng `prefetch(tf.data.AUTOTUNE)`                     |
| C√≥ c·∫ßn x·ª≠ l√Ω song song?       | D√πng `map(..., num_parallel_calls=tf.data.AUTOTUNE)`  |
| M√¥ h√¨nh c·∫ßn shape c·ªë ƒë·ªãnh?    | D√πng `drop_remainder=True` trong `.batch()`           |

---

## üîÅ C·∫•u tr√∫c pipeline ti√™u chu·∫©n:

```python
ds_train = ds_train.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.shuffle(buffer_size=10000, seed=42)
ds_train = ds_train.batch(32, drop_remainder=True)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)
```

---

# üìí Keras Sequential Handbook - Categorized

## üîπ `model = Sequential([...])`

### 1. Sequential Model
- **Concept**: Linear stack of layers.  
- **Use when**: simple, single input/output.  
- **Don‚Äôt use when**: skip connections, multiple inputs/outputs.  

**Parameter choices**:  
- **layers**: Dense (tabular), Conv2D (images), Conv1D/LSTM (sequences).  
- **Input(shape=...)**: must match data (e.g., `(28,28,1)` for MNIST).  

---

#### Dense Layer ( included by .layers)
```python
layers.Dense(units, activation=None, use_bias=True, kernel_initializer="glorot_uniform")
```

**Parameter choices**:  
- **units**:  
  - Hidden layers ‚Üí 32‚Äì512.  
  - Output ‚Üí depends on task (1 for binary/regression, #classes for multi-class).  
- **activation**:  
  - Hidden ‚Üí `relu`.  
  - Output ‚Üí `sigmoid` (binary), `softmax` (multi-class), `linear` (regression).  
- **kernel_initializer**:  
  - `he_normal` (ReLU).  
  - `glorot_uniform` (default).  
- **use_bias**: keep True (unless BatchNorm follows).  

---

## üîπ `model.compile(optimizer=..., loss=..., metrics=...)`

### 2. Loss Functions
- **Binary** ‚Üí `BinaryCrossentropy(from_logits=...)`.  
- **Multi-class** ‚Üí  
  - One-hot ‚Üí `CategoricalCrossentropy(from_logits=...)`.  
  - Integer labels ‚Üí `SparseCategoricalCrossentropy(from_logits=...)`.  
- **Multi-label** ‚Üí `BinaryCrossentropy`.  
- **Regression** ‚Üí `MSE` or `MAE`.  

**Key parameters**:  
- `from_logits`: True if no activation on output.  
- `label_smoothing`: 0.1‚Äì0.2 to reduce overconfidence.  
- `reduction`: how to aggregate loss (`auto` by default).  

---

### 3. Metrics
- **Binary classification**: `accuracy`, `AUC`, `Precision/Recall`.  
- **Multi-class**:  
  - `SparseCategoricalAccuracy` (integer labels).  
  - `CategoricalAccuracy` (one-hot).  
- **Regression**: `mae`, `mse`, `RMSE`, `R2` (custom).  

üëâ **Loss = optimized, Metrics = monitoring only.**  

---

### 4. Optimizers
- **Adam** ‚Üí `learning_rate=1e-3`.  
- **AdamW** ‚Üí add weight decay (`1e-4`).  
- **SGD+Momentum** ‚Üí `lr=1e-2 ~ 1e-1`, `momentum=0.9`.  
- **RMSprop** ‚Üí often for RNN/audio.  

**Tips**:  
- Use `ReduceLROnPlateau` to lower LR dynamically.  
- Use `clipnorm=1.0` to avoid exploding gradients.  
- Try LR schedules (CosineDecay, ExponentialDecay).  

---

## üîπ `model.fit(x, y, ...)`

### 5. Fit
```python
history = model.fit(
    x_train, y_train,
    batch_size=32,
    epochs=50,
    validation_split=0.2,
    shuffle=True,
    class_weight=None,
    callbacks=[...],
    verbose=1
)
```

**Parameter choices**:  
- **epochs**: 20‚Äì50 + `EarlyStopping(patience=5)`.  
- **batch_size**: 32 (default), 64/128 if GPU strong.  
- **validation_split**: 0.2 if no dedicated val set.  
- **shuffle**: True (default), False for time-series.  
- **class_weight**: adjust for imbalanced data.  
- **callbacks**:  
  - EarlyStopping  
  - ModelCheckpoint  
  - ReduceLROnPlateau  
- **verbose**: 1 (detailed), 2 (compact).  

---

# ‚úÖ Final Flow
1. **Build** ‚Üí `Sequential` + layers (Section 1‚Äì2).  
2. **Compile** ‚Üí loss, metrics, optimizer (Section 3‚Äì5).  
3. **Fit** ‚Üí train model with data (Section 6).  
